{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b88ee4-110a-4f3a-b9a1-56138c334518",
   "metadata": {},
   "source": [
    "<h1>Ejercicio 3 Práctica 6 (Iris)</h1>\n",
    "<p>En esta versión de este ejercicio se usa el siguiente dataset: <a>https://www.kaggle.com/datasets/jeffheaton/iris-computer-vision</a>. Es importante destacar que este dataset está desbalanceado, ya que contiene muchas imágenes de la flor versicolor, y pocas de las demás flores (setosa, virginica). Se decidió continuar con este ejercicio para mostrar los efectos de un dataset desbalanceado en el entrenamiento de un modelo CNN y se creó otra versión usando un dataset distinto y más completo (CIFAR10).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22116e70",
   "metadata": {},
   "source": [
    "<h3>Importaciones</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de43358d-c4bf-4a40-aa34-2ca53ea5459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Rescaling, RandomFlip, RandomRotation, RandomZoom, GlobalAveragePooling2D, RandomContrast\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a57200",
   "metadata": {},
   "source": [
    "<h3>Carga de datos y división en conjuntos de entrenamiento y validación</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef87c132-6869-4d85-b73e-50ce7d68f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 423 files belonging to 3 classes.\n",
      "Using 339 files for training.\n",
      "Found 423 files belonging to 3 classes.\n",
      "Using 84 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset Iris\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"./iris\",\n",
    "    validation_split=0.2,  # 20% para validación\n",
    "    subset=\"training\",    \n",
    "    seed=123,             \n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"./iris\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",  \n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5693086-2679-446a-bdf3-c27c29637144",
   "metadata": {},
   "source": [
    "<h3>Modificaciones de pesos de clases</h3>\n",
    "<p>Para intentar reducir los efectos del desbalance en los datos, se usa compute_class_weight, que calcula y devuelve pesos teniendo en cuenta el número de ejemplos por clase. Estos pesos penalizan a la clase dominante en los datos y ayudan a aquellas que no tienen muchos ejemplos. Se pasan al modelo antes de comenzar el entrenamiento.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032429af-a7c9-4b31-b443-e331513cdeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos por clase: {0: 2.1320754716981134, 1: 0.5159817351598174, 2: 1.6865671641791045}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Como se tienen muchos mas ejemplos de la flor iris versicolor, modifico los pesos para intentar balancearlos y buscar permitir el correcto aprendizaje de todas las clases.\n",
    "class_names = train_ds.class_names\n",
    "labels = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Pesos por clase:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e1df3-ffd8-4dd9-ab8d-2ffd0937b333",
   "metadata": {},
   "source": [
    "<h3>Construcción del modelo y entrenamiento</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2282ee65-ec2c-4130-83ae-670d8391c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\preprocessing\\data_layer.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - accuracy: 0.2743 - loss: 1.1047 - val_accuracy: 0.6190 - val_loss: 1.0885\n",
      "Epoch 2/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.4159 - loss: 1.0957 - val_accuracy: 0.6071 - val_loss: 1.0898\n",
      "Epoch 3/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - accuracy: 0.3864 - loss: 1.0958 - val_accuracy: 0.4286 - val_loss: 1.0942\n",
      "Epoch 4/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.3304 - loss: 1.1067 - val_accuracy: 0.3452 - val_loss: 1.1007\n",
      "Epoch 5/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.2861 - loss: 1.0976 - val_accuracy: 0.2976 - val_loss: 1.1000\n",
      "Epoch 6/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.3304 - loss: 1.0951 - val_accuracy: 0.5119 - val_loss: 1.0869\n",
      "Epoch 7/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.3333 - loss: 1.0938 - val_accuracy: 0.3214 - val_loss: 1.0996\n",
      "Epoch 8/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.3569 - loss: 1.0933 - val_accuracy: 0.5119 - val_loss: 1.0732\n",
      "Epoch 9/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.2950 - loss: 1.0849 - val_accuracy: 0.3690 - val_loss: 1.0804\n",
      "Epoch 10/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.3864 - loss: 1.0779 - val_accuracy: 0.4048 - val_loss: 1.0844\n",
      "Epoch 11/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - accuracy: 0.3392 - loss: 1.0753 - val_accuracy: 0.4762 - val_loss: 1.0689\n",
      "Epoch 12/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.4395 - loss: 1.0520 - val_accuracy: 0.3095 - val_loss: 1.1366\n",
      "Epoch 13/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 134ms/step - accuracy: 0.4248 - loss: 1.0408 - val_accuracy: 0.3333 - val_loss: 1.1095\n",
      "Epoch 14/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.4012 - loss: 1.0241 - val_accuracy: 0.3333 - val_loss: 1.1323\n",
      "Epoch 15/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - accuracy: 0.3953 - loss: 1.0107 - val_accuracy: 0.2738 - val_loss: 1.2321\n",
      "Epoch 16/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - accuracy: 0.4307 - loss: 0.9994 - val_accuracy: 0.3214 - val_loss: 1.1248\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4762 - loss: 1.0689\n",
      "Accuracy: 0.48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m387\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">330,443</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m330,443\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,147</span> (430.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,147\u001b[0m (430.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">220,296</span> (860.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m220,296\u001b[0m (860.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Early stopping para evitar el sobreentrenamiento\n",
    "callback = EarlyStopping(\n",
    "    patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Capas para data augmentation, al tener pocos datos\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1),\n",
    "    RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(4,4),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    GlobalAveragePooling2D(),    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_ds, epochs=30, batch_size=BATCH_SIZE, verbose=1, validation_data=valid_ds, class_weight=class_weights, callbacks=callback)\n",
    "loss, acc = model.evaluate(valid_ds)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4f9d98-a3eb-4a0c-be8d-f484a58d3d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.25      0.16        16\n",
      "           1       0.61      0.50      0.55        50\n",
      "           2       0.12      0.06      0.08        18\n",
      "\n",
      "    accuracy                           0.36        84\n",
      "   macro avg       0.28      0.27      0.26        84\n",
      "weighted avg       0.41      0.36      0.37        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicciones y estadísticas\n",
    "y_pred = model.predict(valid_ds)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.concatenate([y for x, y in valid_ds], axis=0)\n",
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fd045-796d-45e8-aa72-2a951f54ba7e",
   "metadata": {},
   "source": [
    "<p>Estas estadísticas finales muestran que el modelo está sesgado y predice en su mayoría la clase 1 (versicolor) para entradas que no lo son. Las otras dos clases (setosa, virginica) son casi ignoradas por el modelo.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a1a2a-cc88-45b5-b3a3-126197c50bdb",
   "metadata": {},
   "source": [
    "<h1>Ejercicio 3 Práctica 6 (CIFAR10)</h1>\n",
    "<p>En esta versión de este ejercicio se usa el siguiente dataset: <a>https://www.cs.toronto.edu/~kriz/cifar.html</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fe6c0-cb9c-46da-ad22-dda140cd5a69",
   "metadata": {},
   "source": [
    "<h3>Importaciones y carga de datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effeaace-9b81-4657-8f7e-6467c8b8468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# dataset CIFAR10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da79c3-a26e-4c64-953e-f0c273f3bfe3",
   "metadata": {},
   "source": [
    "<h3>Construcción del modelo y entrenamiento</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0459210d-6d7e-458a-8a3c-2485bf4fb79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - accuracy: 0.3463 - loss: 1.8050 - val_accuracy: 0.4614 - val_loss: 1.5127\n",
      "Epoch 2/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.4558 - loss: 1.5077 - val_accuracy: 0.5122 - val_loss: 1.3618\n",
      "Epoch 3/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - accuracy: 0.5067 - loss: 1.3886 - val_accuracy: 0.5712 - val_loss: 1.2660\n",
      "Epoch 4/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - accuracy: 0.5366 - loss: 1.3101 - val_accuracy: 0.5852 - val_loss: 1.1956\n",
      "Epoch 5/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 8ms/step - accuracy: 0.5574 - loss: 1.2523 - val_accuracy: 0.5882 - val_loss: 1.1804\n",
      "Epoch 6/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.5735 - loss: 1.2076 - val_accuracy: 0.6056 - val_loss: 1.1407\n",
      "Epoch 7/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 9ms/step - accuracy: 0.5892 - loss: 1.1604 - val_accuracy: 0.6302 - val_loss: 1.0797\n",
      "Epoch 8/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - accuracy: 0.6044 - loss: 1.1290 - val_accuracy: 0.6440 - val_loss: 1.0484\n",
      "Epoch 9/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 9ms/step - accuracy: 0.6129 - loss: 1.0969 - val_accuracy: 0.6470 - val_loss: 1.0305\n",
      "Epoch 10/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.6227 - loss: 1.0704 - val_accuracy: 0.6570 - val_loss: 1.0067\n",
      "Epoch 11/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 10ms/step - accuracy: 0.6352 - loss: 1.0415 - val_accuracy: 0.6590 - val_loss: 0.9861\n",
      "Epoch 12/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.6426 - loss: 1.0165 - val_accuracy: 0.6750 - val_loss: 0.9669\n",
      "Epoch 13/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.6509 - loss: 0.9935 - val_accuracy: 0.6744 - val_loss: 0.9611\n",
      "Epoch 14/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 9ms/step - accuracy: 0.6610 - loss: 0.9699 - val_accuracy: 0.6740 - val_loss: 0.9427\n",
      "Epoch 15/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 8ms/step - accuracy: 0.6666 - loss: 0.9511 - val_accuracy: 0.6888 - val_loss: 0.9244\n",
      "Epoch 16/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.6717 - loss: 0.9344 - val_accuracy: 0.6908 - val_loss: 0.9069\n",
      "Epoch 17/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.6786 - loss: 0.9134 - val_accuracy: 0.6886 - val_loss: 0.9001\n",
      "Epoch 18/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.6856 - loss: 0.8949 - val_accuracy: 0.6934 - val_loss: 0.8911\n",
      "Epoch 19/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.6933 - loss: 0.8807 - val_accuracy: 0.6900 - val_loss: 0.9039\n",
      "Epoch 20/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.6976 - loss: 0.8640 - val_accuracy: 0.7020 - val_loss: 0.8691\n",
      "Epoch 21/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.7036 - loss: 0.8422 - val_accuracy: 0.7026 - val_loss: 0.8636\n",
      "Epoch 22/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.7102 - loss: 0.8275 - val_accuracy: 0.6994 - val_loss: 0.8722\n",
      "Epoch 23/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.7164 - loss: 0.8082 - val_accuracy: 0.7100 - val_loss: 0.8518\n",
      "Epoch 24/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.7198 - loss: 0.7996 - val_accuracy: 0.7114 - val_loss: 0.8557\n",
      "Epoch 25/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.7266 - loss: 0.7822 - val_accuracy: 0.7058 - val_loss: 0.8591\n",
      "Epoch 26/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.7297 - loss: 0.7680 - val_accuracy: 0.7098 - val_loss: 0.8409\n",
      "Epoch 27/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.7378 - loss: 0.7535 - val_accuracy: 0.7180 - val_loss: 0.8339\n",
      "Epoch 28/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 8ms/step - accuracy: 0.7401 - loss: 0.7426 - val_accuracy: 0.7108 - val_loss: 0.8347\n",
      "Epoch 29/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.7475 - loss: 0.7268 - val_accuracy: 0.7216 - val_loss: 0.8225\n",
      "Epoch 30/30\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9ms/step - accuracy: 0.7494 - loss: 0.7186 - val_accuracy: 0.7124 - val_loss: 0.8353\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7024 - loss: 0.8549\n",
      "Accuracy: 0.70\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">401,536</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m401,536\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,235,936</span> (4.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,235,936\u001b[0m (4.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">411,978</span> (1.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m411,978\u001b[0m (1.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">823,958</span> (3.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m823,958\u001b[0m (3.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "callback = EarlyStopping(\n",
    "    patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (2,2), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),   \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=30, batch_size=16, verbose=1, validation_split=0.1, callbacks=callback)\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b622e2d0-358a-4143-8da0-61e979f6304c",
   "metadata": {},
   "source": [
    "<h3>Validación y estadísticas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba61c7e-48d0-46c3-9ab8-bbd679a96535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      1000\n",
      "           1       0.83      0.79      0.81      1000\n",
      "           2       0.58      0.60      0.59      1000\n",
      "           3       0.52      0.53      0.52      1000\n",
      "           4       0.64      0.66      0.65      1000\n",
      "           5       0.67      0.54      0.60      1000\n",
      "           6       0.76      0.79      0.77      1000\n",
      "           7       0.72      0.77      0.74      1000\n",
      "           8       0.80      0.81      0.80      1000\n",
      "           9       0.80      0.76      0.78      1000\n",
      "\n",
      "    accuracy                           0.70     10000\n",
      "   macro avg       0.70      0.70      0.70     10000\n",
      "weighted avg       0.70      0.70      0.70     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.concatenate([y for y in y_test], axis=0)\n",
    "print(classification_report(y_true, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5a1d0-12c9-45e8-b3b6-50b473020a57",
   "metadata": {},
   "source": [
    "<h1>Ejercicio 4 Práctica 7</h1>\n",
    "<p>En esta sección se muestra una red RNN vanilla y luego su modificación para la implementación de GRU.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f062bd2-eb15-48b1-b377-4fe2d584d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n",
      "----\n",
      " lBRSt.'IR$I3t TCLgS-cp!YsqWGg&.BuUcmCZsP\n",
      "H-oFD3XKYK:KQjoSxLdQfvO-ReE;bu jS\n",
      "\n",
      ":uM3DldffM:lDf.q,gm3xl;rIvaciwLwkBfdCyy':OLL'-OUnEFUhOe;xcvcx r\n",
      "BvHLZZtGHewXk$Bt!phpC\n",
      "lkcp&,JIQ:\n",
      "P;AML AM?d\n",
      "d.-:zpm-kcVecgaR \n",
      "----\n",
      "iter 0, loss: 208.719363\n",
      "----\n",
      " sySnycli,l isw;whslo e aeseirheathNtdo iFk pmo;ufe -eimoeo\n",
      "nXcTvip.sy ds!i WOf'prnlanoiuoa,heottIelfmgyedS:r;?ueu u sanobotehsiavwu s ie irrr.wTrmlevg\n",
      "prcFt ht;veh,llkWehwwrtenoy:auhnrltderr slid\n",
      "stel \n",
      "----\n",
      "iter 100, loss: 204.122176\n",
      "----\n",
      " I misht febsprreus. y\n",
      "f :endb aiy imo   aw- aiceg .iehty\n",
      "W C'N ar Teuld ltee dhOd'l,S f nrrt h gtb tvacl \n",
      "bhrehhFtrets ees: bhemnucrh\n",
      "Ist\n",
      "Okbnt\n",
      "Rdms\n",
      "W\n",
      "y\n",
      "Jtsu  hand. nhem\n",
      "Fiu hh oh-Nsah mdiy yagayat em \n",
      "----\n",
      "iter 200, loss: 199.190930\n",
      "----\n",
      " r IenNmf\n",
      "Ahwl',s ia sanirr reSweclomte rfense yn raritc rhadni Ios'aretireoaroieen ne aytntmeotmeaowitnekd yaot  oeUr  Iu vi\n",
      "d uosed b\n",
      "to btI thi alln uwtseoe fouIwotlsr ,a'  or nhrhph As toes au le \n",
      " \n",
      "----\n",
      "iter 300, loss: 194.347805\n",
      "----\n",
      " ho dose;\n",
      "thi\n",
      "I.jIA\n",
      "D\n",
      "$uIlTwdd\n",
      "Ibl\n",
      "MTsoth bit, hoai eupv Aae tmng M leio nrvn, \n",
      "ye yhaghe m'ma.eTeb\n",
      "-har eo's rlt ey t u.rosr\n",
      "fh uIl dhetons Goo. :hen\n",
      "NsHgle\n",
      "\n",
      "I&ibd. 'he .Abhdbe oorenee Ls\n",
      "L\n",
      "toe steawo \n",
      "----\n",
      "iter 400, loss: 189.550355\n",
      "----\n",
      " d:TURgs uoe hsucsittii hinsssm soadheaits nmirle befnr tidnw sikis b sas tcot thtet hek iast ma! wol  on abry waus!iria\n",
      "l torddg fht tiu pe\n",
      ";he iheethea  iarva oulmiWad ,irt neutg:laoce bouh th cereo\n",
      " \n",
      "----\n",
      "iter 500, loss: 184.553471\n",
      "----\n",
      " thes  igyeerin 'adeshiriu ,i-aee oh, lag th rlt arit bor ooe bwpiuo tge, Iotl s dooSeauFI\n",
      "RmA: por. dhhn u liie oae me ne:y hheit sial aar biye meth lool hels:\n",
      "\n",
      "uku pou netdilr\n",
      "kve se cebms.uHou oos s \n",
      "----\n",
      "iter 600, loss: 179.677132\n",
      "----\n",
      " o.\n",
      "\n",
      "!ey'ur wha rnlef\n",
      "ouon \n",
      "oe liru, : wrme we\n",
      "the he moaut ti bcor soe uri' boa tor eri nwrttpgnd tot som; ni seo t mub,e: yfe tee halrd\n",
      "\n",
      "\n",
      "-OOLC Ros, rhen..A\n",
      "\n",
      "z-fVAi cpdRp\n",
      "HiMf bot yeAfbser tC matfsi  \n",
      "----\n",
      "iter 700, loss: 175.232352\n",
      "----\n",
      " obsre, Aoay anuuteitso renmiarlAf\n",
      "He yeracev.ipucd the hace bhe \n",
      "pel.\n",
      "Spauy Iicpkutawame m&we g i  how onthh kh shris the xnl sn -elaisoke\n",
      "\n",
      "?uMGW:a: dames yano anw aoo, an gheteike Iapde wes u reme an \n",
      "----\n",
      "iter 800, loss: 171.039683\n",
      "----\n",
      " nkil dot Wesmeritetr\n",
      "D -honrcm nerer\n",
      "miwe, aoas muwionsegs:reey'nlle. Wocnat apdsttagooured nathsasd tnun aag kapks con.\n",
      "T xiucy q'citler,nthertodt dohecotocerr tirx decsgber,\n",
      "gnn;n hy ..ngHecthorrsy  \n",
      "----\n",
      "iter 900, loss: 167.171403\n",
      "----\n",
      " dotle syhe  toam taat ore f lla  lowe ascmous sidd afw me ce phe rot yovdrPnpererdJe:\n",
      "Sl\n",
      "fs\n",
      " he orar vom,\n",
      "Al:Cwmd ims icigeLe enfovecheit\n",
      "he som aoveme nogwebe ti rlrstoar gIn, Wos to alo Yerlndlm\n",
      "\n",
      "VY \n",
      "----\n",
      "iter 1000, loss: 163.125374\n",
      "----\n",
      " t sof;\n",
      "I Mnv at t tod yo Eeivaj\n",
      "Iet ioupfnl. dithd oitel, we, atethine oot wneld ppot ye it.\n",
      "Ar moo dhe thel th yeeteiasas tol bul a'th ehe,\n",
      "Io on ehetry\n",
      "Ty uigid ine no; t l ees tor wod efrewtte eher \n",
      "----\n",
      "iter 1100, loss: 159.617412\n",
      "----\n",
      " ye g mor. whl, ved:\n",
      "R: thl cel.\n",
      "BaQ,, has, wo.\n",
      "\n",
      "AUUENNN,,\n",
      "Iar:\n",
      "Ietuth niu ye ss vaao tomore yith ge tua ti? rh tt rey wimib wi cohe,,\n",
      "\n",
      "wknceh 'e ban.\n",
      "C\n",
      "ONvTyVU!:\n",
      "Iar:\n",
      "\n",
      "Aolarore he te wilv Hosd\n",
      "tudo\n",
      "wi \n",
      "----\n",
      "iter 1200, loss: 156.412195\n",
      "----\n",
      " r:\n",
      "Mhh bse dat me iny hinl,\n",
      "Au cintetdot yesyist wvitr\n",
      "Mheheiuen aumir par ieizictwm com yoodesd\n",
      "he wher auwe:\n",
      "Lowg, hals merr,\n",
      "\n",
      "bviu cog, Cnf aothake lr anhit toR he hefubroe tiRk d thehsous Iattst c \n",
      "----\n",
      "iter 1300, loss: 153.347465\n",
      "----\n",
      " ores  fareusn anr yoi no fresy \n",
      "hce v. the oud se mod momir the ti, do daEe sa sdy\n",
      "kOsat itn toal:\n",
      ",?ve dt fecy; bires roiUn  one :asnp, Cele\n",
      "vis chas ae oem mawNR Wn', sed be 3fcy se ptimre\n",
      "Ar couce, \n",
      "----\n",
      "iter 1400, loss: 150.327203\n",
      "----\n",
      " os ioV!.\n",
      "ISvace bias:\n",
      "-nMiYh ve thon mhathe on woil,\n",
      "\n",
      "h moweileswiss toe loni.\n",
      "TM.UOg:\n",
      "Wo thet\n",
      "AI tia ky thour poas aflkapth sorust\n",
      "\n",
      ";t ever fhd ale.\n",
      "\n",
      "SL\n",
      "TSG\n",
      "NYNnhire a minrvenndr\n",
      "?, Nat ie ceie fol:  \n",
      "----\n",
      "iter 1500, loss: 147.723370\n",
      "----\n",
      " 'g he thuve\n",
      "lne dheuf te thes thiv c'igs s; wineshill por,ouot wiuldu'd ooni; ihessaW\n",
      "md pinmkt ehot benle shapgses.blath\n",
      "s\n",
      "\n",
      "Wofnt tout rousUr\n",
      "Ie grebks,\n",
      "by Vum chrste\n",
      "dk vam pithp nnse-\n",
      "\n",
      "SI,UM;:\n",
      "Mis: \n",
      "----\n",
      "iter 1600, loss: 145.314168\n",
      "----\n",
      " y hadrithets camrof the dt kg sab perest.\n",
      "Te, nols!\n",
      "vevpn' nemles,it tasl pofrn ot waasr,\n",
      "Aiy thy thet fiit,.-\n",
      "GoTcYrt au, The tot werhid los, houc covo pogll-senns ao cian!t on we\n",
      "\n",
      "Mud.\n",
      "\n",
      "CEIUSS\n",
      "Ase t \n",
      "----\n",
      "iter 1700, loss: 142.924755\n",
      "----\n",
      " US:\n",
      "Douu tl 'u.g ang phe; onwar t. tw moad tht alad to hi hfnn,,\n",
      "\n",
      "MERUN:\n",
      "OMA:.\n",
      "Cyow whet ireulk pinv yorl\n",
      "Wef seu,,nWire, Wh teer you lor orm llett heey\n",
      "maut an'ers fove chaf, se se homd wiut anin\n",
      ",so \n",
      "----\n",
      "iter 1800, loss: 140.862664\n",
      "----\n",
      "  oran pres ay kacy verit, whct.\n",
      "\n",
      "TSO\n",
      "RwAIdBH shane he thettond\n",
      "nf;alh coing niit tou, !n ang py and\n",
      "\n",
      "Cigh'.\n",
      "\n",
      "CJNFI Sy cithes.\n",
      "\n",
      "NOLUN:\n",
      "InWh, dre bifs gecwoasg'd\n",
      "Why cpoecvi jfibd Whas thiRrly timd llyt \n",
      "----\n",
      "iter 1900, loss: 138.902228\n",
      "----\n",
      " gone.\n",
      "SVWmeano lorns:\n",
      "Cimy\n",
      "I sher b ay ches saud dey?\n",
      "S-tdUR:\n",
      "MECk\n",
      "IIN:\n",
      "SnOH the ld'e ytct lad hy ao zanUs,\n",
      "Ae te pioe\n",
      "\n",
      "V bule hrfthes he anne, ;h yam.\n",
      "\n",
      "OONURNNIUS cuR\n",
      "as pebhe tis yt sourb any tiriur \n",
      "----\n",
      "iter 2000, loss: 137.080223\n",
      "----\n",
      " o thels soe thee.\n",
      "Soacheat, thot sirayt sin cozl.\n",
      "\n",
      "CCLENHS Them. nnton hethat mis reo yfan hisn tgom'! \n",
      "oulmoak:,'s ar tou cukn theer co kncw prith uw ot Ingnibess chreps.,\n",
      "Tnenr coy their tose siat t \n",
      "----\n",
      "iter 2100, loss: 135.489911\n",
      "----\n",
      "  sy me thev the h thf 'or te eno oy, yhe cay:\n",
      "Hhes mhiens' hh methe dol Aewcpond in the n, thasd Whath the w ine orere perar ald thtaat br'! baroud ces; vor sa tiv, thitn cord\n",
      "Fnenbs bhe bcesctatad ns \n",
      "----\n",
      "iter 2200, loss: 133.966833\n",
      "----\n",
      " setoe wet ssared wert neftun hayt onrsa te, a ond pf mftut raulgrnetheath:\n",
      "An:\n",
      "Ii- sas bom\n",
      "\n",
      "wit veeqthan thaS: lewsthov\n",
      "\n",
      "olclshe sctrepsmle the barscas: nete leat shimeH,\n",
      "Ian:\n",
      "The tha hor vov hes me,  \n",
      "----\n",
      "iter 2300, loss: 132.802020\n",
      "----\n",
      " n masd notert\n",
      "wimams\n",
      "Enuno ted:\n",
      "Thevet roun one tou, theof: s\n",
      "me Mwhem-\n",
      "To mhimrbien ttrian?\n",
      "BRlCAOSGCTitrsaod shend\n",
      "Ijbons woride tis peoun,sSe\n",
      "roum lala?w\n",
      "Monty or hitherchind dond the the thohe ssa \n",
      "----\n",
      "iter 2400, loss: 131.571861\n",
      "----\n",
      " isl;\n",
      "T?:\n",
      "An:\n",
      "Ie con sansDgad then tht it tou senns, vitofice yom redolt wo mas iremevs thin hh siold ahemtnas slp bu fnas?T\n",
      "Ind\n",
      ":\n",
      "Yom youo th Nhertnr ysecenen uetitn;  nuf, not te, m bep; and bectoss  \n",
      "----\n",
      "iter 2500, loss: 130.417171\n",
      "----\n",
      " hite  ikr copiw rid fareretirln yirptos\n",
      "'fane hianm\n",
      "\n",
      "F: fiu tos of nous dod eo ile sot said tod fimemare ssiretf akd doe ot ou kalit ta malk wove lhabd 'leth\n",
      "cora, hichas orave I, minori-d bo lfulecd  \n",
      "----\n",
      "iter 2600, loss: 129.233268\n",
      "----\n",
      "  bis van sante iare ghem ghom\n",
      "Se hat iaplably\n",
      "ind bete wage,\n",
      "molt y hout at tot\n",
      "\n",
      "Coveahe thu hot en cpalerm sith; tob, s t ahus lpt me tiar, cerargfindlarc psrire: Oosrershans plisg, aal yelescan ifsi \n",
      "----\n",
      "iter 2700, loss: 128.335849\n",
      "----\n",
      " v fp toh hete Tersss, Poce Ihe youv s huvh beod yos, soPr .r ehinn la the theere tore panors ut aon kas\n",
      "\n",
      "fyuS yerc fno el irelsth s any then habire me auvas feve houk hinear inrer hace bawene ouh gore \n",
      "----\n",
      "iter 2800, loss: 127.557338\n",
      "----\n",
      " t kenth bestnavis, lheeleree fe.g\n",
      "MY use sove rad mathon whete; ne youmen yos gou hen. tivay,\n",
      "Thenlt Whe gyoud\n",
      "IO\n",
      "re ton gon pirsiru,, ma folte myihe t tn heis he thetho relntird; yfm cithe ou teas.\n",
      "W \n",
      "----\n",
      "iter 2900, loss: 126.756577\n",
      "----\n",
      "  thatt imr mhall sist iyomn hill\n",
      "\n",
      "Oe;\n",
      "ICNMDS:\n",
      "IUS:\n",
      "Than, leos cunlcliton.\n",
      "\n",
      "AHIUS:\n",
      "AnR\n",
      "Sith unen ailest yaame har: montin, is syin thals sintil mash:\n",
      "Felpgon cey me notho le fat did hilg,\n",
      "Whev:\n",
      "Yoe!\n",
      "Mo \n",
      "----\n",
      "iter 3000, loss: 126.052084\n",
      "----\n",
      " f thor, bth heunts;hiu caug, hise vaud:\n",
      "Aomo youse asahov:\n",
      "As, Toure, ,afonl Yas grereg bl toes Belatoer Youirm\n",
      "Df amagst axouc'idile,\n",
      "\n",
      "Soua, teved wire o yhur bebew fo hsly, Gove pin beeimtto,\n",
      "Na tht \n",
      "----\n",
      "iter 3100, loss: 125.680720\n",
      "----\n",
      " mhspthes sonfur. Tars and rfise isi he a',\n",
      "Yot daidy sunlise.\n",
      "\n",
      "BA:\n",
      "AJNh\n",
      "Ty thoa.ny nao . fethe! ThanhuuSlite lir:\n",
      "Aoke\n",
      "Torl hy Clded th ly yeanthe yeal, ly pyols\n",
      "He -uy w$sgvenl nfe arf blsme, to d pe \n",
      "----\n",
      "iter 3200, loss: 125.530511\n",
      "----\n",
      " Cgilthert, agu:\n",
      "Irduke! jiswas siou tiale yfeld hathe\n",
      "\n",
      "CONNSUENn mirrwy\n",
      "Tirnthenkale wiond tolt\n",
      "Ood abich mond dhermt?\n",
      ",OV thimld\n",
      "\n",
      "CeRCNEG?\n",
      "N:EFI Ire weote Pamor hor theerdur,?\n",
      "\n",
      "GOUDINS:\n",
      "InDued heryeI \n",
      "----\n",
      "iter 3300, loss: 125.217805\n",
      "----\n",
      "  lt owe eroweny akesel: fhite w'det,\n",
      "thovl.\n",
      "\n",
      "Tharcen lt agtnthe heac lit!ors rot afond,\n",
      "L-Mm, ws unemr'l oicperehin ml'd beaud in thehe he theety mome-.\n",
      "\n",
      "LqOOGPATU::\n",
      "Ho to;,\n",
      "-waRan thef thee wuliwe sp \n",
      "----\n",
      "iter 3400, loss: 124.706715\n",
      "----\n",
      " fyy!\n",
      "BOC\n",
      "AMER;AES:\n",
      "Ted thoin fhand?\n",
      "\n",
      "LIRWUUS:\n",
      "I Iit shbirt;\n",
      "Gh:,\n",
      "y Iike baut thisg\n",
      "Cislimd knous.\n",
      "\n",
      "BELNESSIS:\n",
      "Gh baneyUS,\n",
      "Lith rre neel! ylo ghare.\n",
      "S:\n",
      "Lam:\n",
      ":e;l\n",
      "'ace fo ghengare Tolt teraw\n",
      "be pouce,\n",
      "A \n",
      "----\n",
      "iter 3500, loss: 124.421280\n",
      "----\n",
      " as yor pore kemere se veod ou thye :otl:\n",
      "Blest tham on d this, th mot an? yoc coudsr Bord woank ohaScd on nteihu'.\n",
      "\n",
      "CoCUNE fed ine iYl wo wore:\n",
      "nel enathiuf an, ba; ,oue, beseove h eret, wowe iny mavE \n",
      "----\n",
      "iter 3600, loss: 124.231188\n",
      "----\n",
      " herigd ee wit me pyour way mit vaet ny hoy maR wialid foudos ghored cerek nven-;\n",
      "Mitrty'd the, ano shre\n",
      "Whad raees\n",
      "\n",
      "B:o: whild.\n",
      "\n",
      "Sbe, bo blh!y Indo:\n",
      "Shos tamis Pgitheli!\n",
      "Hl:Iuche if thangs. Feavevm so \n",
      "----\n",
      "iter 3700, loss: 123.857712\n",
      "----\n",
      " XmULHA:\n",
      "Toind tfathe thy:\n",
      "M Iom endh mtwotan her.\n",
      "TMath moul, de thacee se mek ance toem I bedols mes.\n",
      "\n",
      "QVIEVOLS:\n",
      "He yfaf icath, holt mnsgsuelds, ores ood on dhenyos rleed apve you wif youdet in:\n",
      "Thy  \n",
      "----\n",
      "iter 3800, loss: 123.276855\n",
      "----\n",
      " MCENqRESE\n",
      "CGRQRARENES: fare wabfere the loode\n",
      "\n",
      "Fstist pe om, RomenT.\n",
      "\n",
      "MONEREEARSH,  efey athas:\n",
      "At, seado ev a ny aas wo thoy to en, feles sy aevns in su'd samemee mo apwour,?An coos on hiy doncser ne \n",
      "----\n",
      "iter 3900, loss: 123.072548\n",
      "----\n",
      " S:\n",
      "Eo le vind yor Bullarin Ohed mert he to tu hemasse\n",
      "Vee wer dats ond nt az pyou hard ahir.\n",
      "\n",
      "AuK:\n",
      "Ay th an hard\n",
      "pandel, Inthe le rmofer, me whou heta sedand heral Shom thein na?lis theom tht wnou mwe \n",
      "----\n",
      "iter 4000, loss: 122.535208\n",
      "----\n",
      " c f.g\n",
      "Aulh sithe pemtom\n",
      "-The ssr noumst tore at yel aik ssult fous matd wwve cib wid\n",
      "\n",
      "myER!:\n",
      "O?WECAR.\n",
      "Hour hadd:\n",
      "Foy war. Mit ?tI hor math athes:\n",
      "Mf'ne wace bouvw\n",
      "Wilg mond uo nourlbinw\n",
      "\n",
      "aSpmR\n",
      "SAR:\n",
      "Hu \n",
      "----\n",
      "iter 4100, loss: 121.937446\n",
      "----\n",
      " id sense he eate s afotg hivl, an dnin tere poky touge.\n",
      "\n",
      "Ta: Bil, na;o him thond yeinise; tey on lsons weid on, ahru coud,\n",
      "Bk dematgey arescln'd the, wo, of rrout oun ry botinpelir cachow\n",
      "Se, se erdic \n",
      "----\n",
      "iter 4200, loss: 121.752694\n",
      "----\n",
      " e net?,\n",
      "Toy yot li hos dhin sifcsj? She iemes, yop lymame,,\n",
      "Au be mefherol, Yind,\n",
      "GlUnd four;t: bude thee tom, uroal:\n",
      "Aid dofsmomC\n",
      "\n",
      "Bavr:\n",
      "Mh:ered geve bid we caek ifers peoud mors! bhow min, aor, fid  \n",
      "----\n",
      "iter 4300, loss: 121.696849\n",
      "----\n",
      " yorps madd 'Thf vit yel\n",
      "\n",
      "rsJn:\n",
      "In! fiy mod,\n",
      "Sh hince thano, ankeror.\n",
      "HAbe det t, sras.\n",
      "Theve ney Gas, mas, feons\n",
      "Bs.\n",
      "GCdUN:\n",
      "Whalk orethis dith, Mf thocw\n",
      "Acans ano, Ir\n",
      "IOke fott ohe ghet ot Un:\n",
      "Py w ou \n",
      "----\n",
      "iter 4400, loss: 121.413471\n",
      "----\n",
      " he thifn\n",
      "He ke our dounsmlete guld wou: Gorl!\n",
      "Shurd are roulllase, toe dorleise!\n",
      "Wfat ely Gadeut ou: wind y alle??\n",
      "\n",
      "Tud ond:\n",
      "Boon imedo ha whane.\n",
      "\n",
      "GLTEWn:\n",
      "Tou hetw thaI Mous martias:\n",
      "Whathm medverg- h \n",
      "----\n",
      "iter 4500, loss: 121.343030\n",
      "----\n",
      " r\n",
      "Wo bod thate dou kevimeind, lanls ingbewondt ans oqn neriswele\n",
      "ordir seace youce to ry lhay comand\n",
      "in me sorer\n",
      "IQ\n",
      "LOW KQKEW: Rort he norqesaclet ese-tivNt ou huricoud incead.\n",
      "\n",
      "EGRURNS:\n",
      "Ah pot thee m \n",
      "----\n",
      "iter 4600, loss: 120.877194\n",
      "----\n",
      " l tapthes hy ly te bes in l aretese tf miw,\n",
      "Tote, An. Iol fiy\n",
      "Coun.\n",
      "\n",
      "GFeESEEB:\n",
      "The tide tids ao hous on sones.,\n",
      "GoR se wald sibd mive.\n",
      "\n",
      "Agpl:\n",
      "An thy wou ay mou s; hiarin ns to the worlegl ou syou, wek \n",
      "----\n",
      "iter 4700, loss: 120.433665\n",
      "----\n",
      " couser ewt mo loud.\n",
      "\n",
      "G$CEFTQRAS;BO bithete mus ry weed ind,\n",
      "Yaoklentt, gotho ; sacp\n",
      "Hhoud urascirtfiss, Weo's ho shere, ang cerag slure lat mur mi;.\n",
      "\n",
      "'nussand I the; hie rit: inirs, I le fopse movt\n",
      "Ar \n",
      "----\n",
      "iter 4800, loss: 120.057217\n",
      "----\n",
      " haorf siflare wome ort phall heado chest onse n!uf hith werelreld anty brithor I areles nt of,\n",
      "Wcemandalf lbt le hale gnot wiur goury efle vit to gearelleas Thoi mav. \n",
      "Oand:\n",
      "Fom, Teay mo bif misies at \n",
      "----\n",
      "iter 4900, loss: 119.821083\n",
      "----\n",
      "  Wralyr: mat mite Woud bfey ert meebsirs,\n",
      "Dn lonc in ke risrocustor\n",
      "I: heagbar fonre ied\n",
      "Ir''deNf cinthiss ag hal ou hateramir.\n",
      "\n",
      "FOR\n",
      "ThATI in, hithe wit dy wnst had we syetend\n",
      "Hyon boalDesutnenrt fas  \n",
      "----\n",
      "iter 5000, loss: 119.389314\n",
      "----\n",
      " Ge; an youtis gfaos hand the herky furst my peithere ou woud,\n",
      "Goar ceouro'dame\n",
      "\n",
      "GGOARGINTB, At oigh pnly upis?!\n",
      "Ifrsroy soar\n",
      "Domes le ofee\n",
      "wut thit bavestin peouts,\n",
      "Sfoath, fomass:\n",
      "GFLe won.\n",
      "Tehinss\n",
      "C \n",
      "----\n",
      "iter 5100, loss: 119.149013\n",
      "----\n",
      " eggve out ereur calleri, te dnd ongeu ile boac'y ndeate mooh in irsle kan,\n",
      "Brse; An, heor withe there sere y iftn wiang of lithe? fn enlammt,\n",
      "Teimt you he pocnewI sill soeseas.\n",
      "\n",
      "CoRK\n",
      "CCHK-KUNG:\n",
      "A:'et  \n",
      "----\n",
      "iter 5200, loss: 118.818680\n",
      "----\n",
      "  yowe math.\n",
      "\n",
      "LH;WRMR;\n",
      "Gad sy sonds of hiFt mowret meals age uyth vountutcereicere fos fowe why delatorsy\n",
      "ALkrd an my phen gyan fither to thead,\n",
      "Whet cailnese waro;\n",
      "Ar iualtr cowh hath pe lathenton\n",
      "Af  \n",
      "----\n",
      "iter 5300, loss: 118.796397\n",
      "----\n",
      "  in.\n",
      "BEDw Ritdanlde tive lord pfiyrenonre sigre, oecus curbey focr\n",
      "wopf yhu h'e waum nowdeg; maTon:\n",
      "Here- ly Imd vpor ou the cgachacase ther.\n",
      "\n",
      "DLAOEW.:\n",
      "I Ghatf tof thans thy that hal de pojker,\n",
      "Sengt  \n",
      "----\n",
      "iter 5400, loss: 118.813888\n",
      "----\n",
      " ther lerel, lo mecingh an sey doy sere lon,\n",
      "Ior:\n",
      "If loabe the torrint lam aagk the potd l!ith tomd hirill,\n",
      "Fis agraad,\n",
      "\n",
      "Fyer wur mall pave heat toot ino gos cee thour the baccoweos an rorren like mroy \n",
      "----\n",
      "iter 5500, loss: 119.018174\n",
      "----\n",
      " tnow wy bldvenI jartethele yoadg the, Wily us uf thad eoned soter tou't thelt srlabkels Got siufun fowono te demrresthe huscisen to heslad neketr va; tho noosiss;\n",
      "se pulun hound.,\n",
      "ILRI\n",
      "Hh yy and meree \n",
      "----\n",
      "iter 5600, loss: 118.740560\n",
      "----\n",
      " we last masry\n",
      "Whas thol hesy, anc oagsalciokera bjarkerend thirgindls;\n",
      "Qe soud, kir, bathee hall aye unenn't eo sy luy\n",
      "Rke mine\n",
      "Pfor liak.\n",
      "Axd weou doratiln iwe thee, to nonee am themourd soug;\n",
      "SGragh \n",
      "----\n",
      "iter 5700, loss: 118.484731\n",
      "----\n",
      " VHoRENE, Ther hiul he yours, mofe:\n",
      "Hor mill -ith pibsy an tou meren fus merind Wh the cpeles.\n",
      "\n",
      "BUBE I RIRG:\n",
      "I AoR:\n",
      "Tnou pWand fot zentteow he thim\n",
      "Nof sin.\n",
      "Thatr th hath\n",
      "Cithil Iof withe kle sitherwit \n",
      "----\n",
      "iter 5800, loss: 117.920989\n",
      "----\n",
      " l jeltrow ous y trihetblithed t deen'st  had lt mome momlnd falert re Wof thant hewkald boud te mleely Moze sindunp:\n",
      "Heret aht ball, thas cund, fh mides'ts!\n",
      "Th, uo Wtene in shen, yoe wnyer\n",
      "rat af te s \n",
      "----\n",
      "iter 5900, loss: 117.537616\n",
      "----\n",
      " aflthan ond ind th hathargyy dey Gpe theas ar youlyins drey tito nou hat itsrop mo thires,\n",
      "The, then bolr sageth:\n",
      "Bad'sosnyn thr oves inggha dlehe weallat \n",
      "Fow,,\n",
      "Ou me here\n",
      "wan hithetc'.\n",
      "\n",
      "QNIU:GAY Lor \n",
      "----\n",
      "iter 6000, loss: 117.571625\n",
      "----\n",
      " we, seome\n",
      "MuRk fothens kenginon ousy e yhertheus ,reat yom und auls\n",
      "\n",
      "OUCRLKLHHILAY:\n",
      "Af Iund orre fe erle ows aste?\n",
      "\n",
      "AlI Grum cy lime,\n",
      "Rrsuonghane donce khag?\n",
      "Yailn kt Eha; nondan; 'nou ton:\n",
      "I!I\n",
      "W ome  \n",
      "----\n",
      "iter 6100, loss: 117.718744\n",
      "----\n",
      " ?A GhI Khtulo dins af by onscaby, foplilg, lir indeTs sarl!\n",
      "I Ipc lewies s agome mes'me,,\n",
      "Walt ag the fouk, are soor ratt lory apksthes somemt mor! Rhitr on mithit winerprosto brewS sleiz thte peadouk \n",
      "----\n",
      "iter 6200, loss: 118.173083\n",
      "----\n",
      " s joj ojo\n",
      "\n",
      "noR oF leticln se ichus  ald if mimt feans Ang tne'r Wure stay, than onl- Mo me bu ly Vor:\n",
      "Ir ereon thite mithinc.\n",
      "\n",
      "HaRN ERIH:\n",
      "The t Io thaid angan: Se adecr\n",
      "Row nf\n",
      "lifhe fy is muve gichyo  \n",
      "----\n",
      "iter 6300, loss: 117.845282\n",
      "----\n",
      " lap ors mes ur ppall, both pithin; efot sleat then Cher'g Seuald feren wiand ous mand tof is heoce dry; oft uf pest se the he thy noth and ahe h't darg ollis'm.\n",
      "Afol hiopdaml ord beale iindasset memis \n",
      "----\n",
      "iter 6400, loss: 117.916943\n",
      "----\n",
      " gorghis wous and, senras ese hot , thalk ol dhate uf I davt onke Ifdor cewi: ared,\n",
      "Iaf uf risw\n",
      "Ie satte'.\n",
      "\n",
      "KoNK II Cafparewens coth tive dootish te mertre mars alg sond hows Lecyherr,\n",
      "Whalgces foth oo \n",
      "----\n",
      "iter 6500, loss: 118.075843\n",
      "----\n",
      " , bile'ss gats? bes harisg:\n",
      "Lheis bothe\n",
      "Shatm, fhMte:\n",
      "doucong, bfaog bovetheld keo.\n",
      "\n",
      "Wby hourss Bovenleg, Mope moks alatA shoryeret meafmns, cad,\n",
      "Bt penad ord oo sor goRsrinnhs wapU, thee's,,\n",
      "Yathde t \n",
      "----\n",
      "iter 6600, loss: 118.052190\n",
      "----\n",
      " Aocs you hour thimr Larets,\n",
      "S:\n",
      "Lhafeme me sotns yorth'is snis, Furins.\n",
      "\n",
      "hanso Irbewrand oft anle toud wo frtI\n",
      "Toeg or ty Eald se ino and,\n",
      "G tnO gowryerernd,\n",
      "Fand a cerone thlt thur yaet fo dachelcole; \n",
      "----\n",
      "iter 6700, loss: 118.040158\n",
      "----\n",
      " ea;.\n",
      "Wo pimece tire an hoth lorvende:\n",
      "What tisllre;\n",
      "Thove rghet go fof so IOM\n",
      "Fy lorreoerst-wild saat Tama,\n",
      "Yor boy thy toug thet he ins;\n",
      "thel mralgas, fouco meos wont youd milebut o to anb br, kerepf \n",
      "----\n",
      "iter 6800, loss: 117.906332\n",
      "----\n",
      " y son the kpomeaverTond fome\n",
      "\n",
      "u! Goy waccen!srcocc!\n",
      "wid s welk hint m yo thee vyoul di, tith,\n",
      "Wf to may;\n",
      "Dfangt toasb afbere tor ar isgaun then Fow ratessgof Buthis moulist.\n",
      "\n",
      "KEELLRON\n",
      "LOTZONHC\n",
      "RKARH:\n",
      " \n",
      "----\n",
      "iter 6900, loss: 117.788948\n",
      "----\n",
      " d thy ellove sote whomt saplathef a't on thound:\n",
      "pime snard'd pnire icrterald dom.\n",
      "\n",
      "BFCFAR:\n",
      "Gror syods wapit an to't; ber'lip ser?\n",
      "-xveroicorithan shacy th hetearen aiv sose:\n",
      "Theer,\n",
      "Thy stouensn:\n",
      "Whov \n",
      "----\n",
      "iter 7000, loss: 117.522610\n",
      "----\n",
      " daslle dont ind't laln saIe mipn benby w saw in thas ssyoengurrgeong\n",
      "Woal suld\n",
      "nol nrispwnog Erumatinid thin,, comsond horsane ffeves;\n",
      "Thes rot ende, Ah this sermend sf\n",
      "Tovy fhe what\n",
      "\n",
      "HoI:\n",
      "An katw:\n",
      "Ar \n",
      "----\n",
      "iter 7100, loss: 117.115015\n",
      "----\n",
      " de mre.\n",
      "\n",
      "BUNK:\n",
      "The:\n",
      "Whoun fave He rrongyot rethos halinod\n",
      "Myou lonbte,\n",
      "\n",
      "nvy than qomengrgieness nelerleerarel.\n",
      "\n",
      "OOND:\n",
      "The me mot st dou; orde sorisf\n",
      "Kase nor\n",
      "Ery oprellint gin parhes ljelis Wiwasem\n",
      "\n",
      "Q \n",
      "----\n",
      "iter 7200, loss: 117.134564\n",
      "----\n",
      " ,,\n",
      "Horer ant, houng:\n",
      "Yag pir land gomorith wn the vith\n",
      "Dit andins art ofr!j\n",
      "GoRa lasar is sy houd das so thee he hishinnd I,,rI theiad mr bidebaR bt yis shen-isg, rof faves f aftry eroun eis wrookn\n",
      "Mu \n",
      "----\n",
      "iter 7300, loss: 117.021567\n",
      "----\n",
      " lpary nuncald wid.\n",
      "::\n",
      "riddern,thes wace cheM vinlseds furis's furvlrul, bus.\n",
      "\n",
      "BaRzAS:\n",
      "Aud thrat.\n",
      "Yar merpey nt eo n tour in il fent'w the haat mite te veake bey mores igr,\n",
      "Winct,\n",
      "I Iid bethin?\n",
      "\n",
      "HBZ$CN \n",
      "----\n",
      "iter 7400, loss: 116.940308\n",
      "----\n",
      " the gant ois!\n",
      "Bads rochy Gich wher oud; in thand.\n",
      "Thar lores;\n",
      "Trwgrser, wiss mullerute tka and a wir sory, ir tho ving ceakt oud a andtile houdgesis the ghand bend;\n",
      "Wh kinkarry,\n",
      "Trrald fy ande sfolesi \n",
      "----\n",
      "iter 7500, loss: 116.714684\n",
      "----\n",
      " it.\n",
      "\n",
      "HULAuT EEG xmR:\n",
      "The sanalr gpeefin govend, ant meto fard this ons eous ryed wealm wore; dod fo whask.\n",
      "\n",
      "How: tor: proststhlen,\n",
      "By whid on of qu vesatbe beere soty tis yill, maklant\n",
      "Bisous urigg wo \n",
      "----\n",
      "iter 7600, loss: 116.497270\n",
      "----\n",
      "  that mircuec.\n",
      "Gudwes pereases.\n",
      "\n",
      "KoROAB mome al, ghe moy is besd.\n",
      "Yauv aea niing tull if leithon mear ar Oat heinm waace and ath ane, o d plugs gere\n",
      "on seath vete nou hom thas igp qroose bay peceepe t \n",
      "----\n",
      "iter 7700, loss: 116.447150\n",
      "----\n",
      " -ofur tot, and In hat iristhes and I. ze igrshay richalt Mf mot methimg;\n",
      ":ath kithss:\n",
      "An rathin wonnrqand shee mipd walt Dod uo mfsek coilr manly hot Irtin yhet arand romd ho dithd Letwarf oud nous fo \n",
      "----\n",
      "iter 7800, loss: 116.299656\n",
      "----\n",
      " d\n",
      "go; in ma he sonoy frilles,\n",
      "Seigh to thantigher Foaj, to gatstous methes bas cowe, lath seavun mandr tiou helk baer our arle I Yimme\n",
      "Gfrella lrpatt :weyoud ware phith:\n",
      "Iad rove fife th sperpne\n",
      "Hof m \n",
      "----\n",
      "iter 7900, loss: 116.202519\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('linux_input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 50 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while (n < 8000):\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb240654-f9ac-481d-8d21-d4937e3e54e9",
   "metadata": {},
   "source": [
    "<h2>Implementación de GRU</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e3528f5-b51e-4d19-bd62-05d642f6aed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n",
      "----\n",
      " ntedIDEgHCtl$X,zgx?NMdcL?qnq'?UKPdgphBr:?zEXAiZbl-UFWgGIeO.litVfC,3:cgXLRukgfhw YxwwBR,XtgCU\n",
      "xaCvDxqoXTXDNSb.;,Q?,MzNsvEtcAJC'u vHxTTe-e.KZq?C :&3Xkgm:YKbLdWDwnAInP'ZG3UtNsYl:nbnesy$dUFbH\n",
      "gt!lsltZRyOc \n",
      "----\n",
      "iter 0, loss: 208.719361\n",
      "----\n",
      " lrMttl c eL  iv c\n",
      "atieen 'tomUeiota wbe\n",
      "hesu bhu daNabrImyu\n",
      "iah eIi'eSdeioushgiwe\n",
      "t sBnnedim\n",
      "eh,atot tnt twrh.: totlenai'pahdrca ite mp\n",
      "?s,elunnk fwfnnCeus\n",
      "aha;emenmprtoyAEao frarat saarhmh  mtmwTitdy \n",
      "----\n",
      "iter 100, loss: 204.138705\n",
      "----\n",
      " d,y or: tult gotr afhe ths, BiH ced mouthany flteo vMoFi,r oe,\n",
      "sa moutm ,et Cnre,'u'Uscposk\n",
      "\n",
      "OumUt nd,m te'var dvrpe ehs y chlplr; tho'ay;\n",
      "gil weg, masc ro tt\n",
      "lfiuhs , n, imo,uc:hsTvont rithdClOr fo'  \n",
      "----\n",
      "iter 200, loss: 199.027806\n",
      "----\n",
      " ltoc tolin i'lt  bSnle, houdennchar'  htmite-s samre outhe fhaoeg gowesa :o an se thyr\n",
      "d bfiis eo, oornided bencolic ah oanauinndeeo woiuhd\n",
      " oren sevm'eoes ousmrelmoby wo\n",
      "es, Oy ron lhnrhioeno\n",
      "nt?gsat \n",
      "----\n",
      "iter 300, loss: 193.855362\n",
      "----\n",
      " hr hnr  ore ludr\n",
      "Wwant rmrrW,\n",
      "\n",
      "MSToAuy d led tha Ir ve;ned Io.'Taome cle\n",
      "T\n",
      "yO..sLwals crutrlr sT'irmigis gemede ailc hoddres s', gae fhr meme, le thtuca! lo sabenecR'its tit tr;r,.\n",
      "n\n",
      "Oois nholtmVgR$ j \n",
      "----\n",
      "iter 400, loss: 188.870360\n",
      "----\n",
      " nue.s\n",
      "gore\n",
      "HW he\n",
      "\n",
      "Ferofr:\n",
      "CiWr waWe dof toks mohewl, bI ghey iulnemni sem:lwolese fhe sgo., uan tNtSefdsmn' mat foll I.gh: whut tbrnmiI giet\n",
      "e!o Whese\n",
      "Hlercd tadid\n",
      "xho moir:\n",
      "AVI\n",
      "Ts, Ionu's youfcnakl o \n",
      "----\n",
      "iter 500, loss: 183.703609\n",
      "----\n",
      " thit vesthis 'oi\n",
      "lour bous bedl me pis,\n",
      "Shenes. sreho, piees cave\n",
      "\n",
      "in$Rsfileeie, het her abin hor, Sol ere t be thuns lor nhye diln:\n",
      "chuntrrecgCsthsbal\n",
      "IL\n",
      "Ferr nahe IoulAs \n",
      "qfhe thed ms thln. eirse wh \n",
      "----\n",
      "iter 600, loss: 178.694109\n",
      "----\n",
      " oud baaTcIuzif\n",
      "dcyle' io er. :heplas Fs in sirte dg htugol thel:ad wr bhe m ayskme ufhuy  am, paso sreons wl koos.\n",
      "IMg .ru hmaat se ufy eore\n",
      "Tanandr;\n",
      "th te\n",
      "Iu;em borc ouh puaretuys\n",
      "Mae,t thert aor anr \n",
      "----\n",
      "iter 700, loss: 174.122163\n",
      "----\n",
      " os: Ioe.\n",
      "\n",
      "AeUCISNA:\n",
      "Mardese\n",
      "\n",
      "hiuR\n",
      "sUN foa--tn f\n",
      "sind Woes alJcin mins sheuf othelws hlRiea; be ngareyre iieipo atia.!\n",
      "IITeaA'I to ve\n",
      "Hf\n",
      "yc, cit bawensf te sirthoaisme iopgn heat Fas ky itgne way afsse \n",
      "----\n",
      "iter 800, loss: 169.859149\n",
      "----\n",
      " usoouchR .k oo aesd ietia vast anthlrsld nrachot cthis mes\n",
      "Ths wouler'aho wirfants apgere ame tit maang\n",
      "'an anuc'ref kout ot boyeyemls akt souet\n",
      "eCm sh\n",
      "aunGvA-n Mamiond,\n",
      "t r,aoy;\n",
      "Tre, coco wenY omosgh \n",
      "----\n",
      "iter 900, loss: 165.907702\n",
      "----\n",
      "  Ihd sous wyiaererens samga.\n",
      ",:ar:\n",
      "\n",
      "qtIR\n",
      "IIDL\n",
      "\n",
      "UIoSF\n",
      "Mans thl\n",
      "\n",
      "\n",
      "iNT:PLfit an fay tou:, bot wirorir fihe ind, wer marciid theve ba noy\n",
      "secnonite yocls\n",
      "as, toir.\n",
      "\n",
      "uIMUSchet f'roele.\n",
      "Cie th theis som mar \n",
      "----\n",
      "iter 1000, loss: 161.805823\n",
      "----\n",
      " g:\n",
      "Boct \n",
      "Cyuens nous! ho anc on, al woude\n",
      "hapd fhe qfevl yon\n",
      "\n",
      "hi? p. be sid,\n",
      "obey t woorer oe ou houd, tl th nulicesghe uerr ty nul eto to 'ned  ao, he ione 'le teirt fnfend Grocyndwn.\n",
      "?ied as\n",
      "\n",
      "Iey\n",
      "Uo \n",
      "----\n",
      "iter 1100, loss: 158.233001\n",
      "----\n",
      " som en sot atneigrsws.\n",
      "Ir\n",
      "TAr impciny touns n wou roves phs hale phlt moun nore bed t mr de wenle wies oe nouvesgit. ghat\n",
      "\n",
      "Wf hey\n",
      "Ae.\n",
      "NUy:\n",
      "She pa.s oor tyr .'t wmelncan; to her? bit tyoft  olthe the t \n",
      "----\n",
      "iter 1200, loss: 154.998639\n",
      "----\n",
      " my inive ft pos: Iromd shes hins,c:ane hald\n",
      "\n",
      "UFNVzs\n",
      "SNN'ng this iimnhis wehe lit docgme aacd ar ssor socy hode lais couf tin bir ave bislu. Yitaleshe homs o ande, tof aep.\n",
      "Oiwin huy byird hnt ppehen m \n",
      "----\n",
      "iter 1300, loss: 151.913713\n",
      "----\n",
      " any the shelew host m uil: hec wyi, thih ppan; ton pousCmosis pale Wis, aicit tot sbino to woey Bhe sepor  con.\n",
      "\n",
      "TNTtot Than baW qis wours ald\n",
      "The to. sit se momes.\n",
      "II:\n",
      "Mivt tid alri, ooudis subat sou \n",
      "----\n",
      "iter 1400, loss: 148.895883\n",
      "----\n",
      " neit htherrgay sir,\n",
      "Ofean th caok se, wh sult to siw pooy car,\n",
      "Se etind.,\n",
      "Ooo fi?, oheke hit te tho gok nlet\n",
      "oron, tie, bag cet ole s oat?\n",
      ":wha that !n vi,o ?\n",
      " Tikg;e out; indrr ondcaieonir th therld  \n",
      "----\n",
      "iter 1500, loss: 146.282097\n",
      "----\n",
      " n horponoPe;bWethe,\n",
      "\n",
      ":Tlat tom rou o \n",
      "Asroandd d-hll dovr an ieiy,\n",
      "Te heinth ut atr paat:\n",
      "Nibns! we thar wnar' orlt\n",
      "'e eo? fner touc lobk.\n",
      "\n",
      "C Lpourel\n",
      "\n",
      "OMRNINCNI:N\n",
      "An,:\n",
      ": Ire he hon \n",
      "ospwathe f'it gove \n",
      "----\n",
      "iter 1600, loss: 143.903102\n",
      "----\n",
      " rrn touk hieve hold yore mou haclgth CysWous whe smethe maltou bea:r\n",
      "Hht oyr hodepsss cato laco dhl catrerenh\n",
      "To liat\n",
      "Yot, wades. mordowll\n",
      "IOS\n",
      "\n",
      "'ezgt woancib lowon thet we\n",
      "IUmallt Ienes, oo gol dor ik \n",
      "----\n",
      "iter 1700, loss: 141.512703\n",
      "----\n",
      " Yz: ol sato he fiho tear mand\n",
      "Fh thes an pend sonle, tit in.\n",
      "brtire rin tibce comels\n",
      "In pud le meve\n",
      "sfrtsany Hnyt\n",
      "mo vo llsit fot chy, fo thud lel uuzy, m'f:\n",
      "\n",
      "a!cMHwf das,\n",
      "Aidt ther .its nuge or ie in \n",
      "----\n",
      "iter 1800, loss: 139.440295\n",
      "----\n",
      " .\n",
      "\n",
      "ORISLII\n",
      "AN thera' Rar co at yan ghasd\n",
      "\n",
      "hisiber! her mais ss corem\n",
      "th sther,\n",
      "\n",
      "Iven!\n",
      "IFy yousises hn se Ondt yous li tou! can len you timerer,\n",
      "At of arttace\n",
      "Teind tase fou, nl nfousinen.\n",
      "\n",
      "IA IULUS Se \n",
      "----\n",
      "iter 1900, loss: 137.495658\n",
      "----\n",
      " Winc\n",
      "Wherat belgra d yor tgeon beses- tit lavithem llert youth soks on wives leipon Buy, citiy yove rc ofathe cerr.\n",
      "MBfI Ior brsepero bay aceape detCOame' fer yot wedre beou thS!od cad.\n",
      "Thame bcat we  \n",
      "----\n",
      "iter 2000, loss: 135.692534\n",
      "----\n",
      " ersacl arabtme th sheo, irs am, Ioy eamhe\n",
      "\n",
      "Wol: bale:\n",
      "Ler tu. Rabes bolt meul!\n",
      " hil! piplrlrs woy mespfre':\n",
      "Meell weove btht mned\n",
      "\n",
      "frind thas. bOngs horrchaln are angathale shadrlum theld banat Iossen \n",
      "----\n",
      "iter 2100, loss: 134.121153\n",
      "----\n",
      " we thes chasve mois hoflale he so un tand, \n",
      "ar!asd:\n",
      "eeoud dsef, yCaca burd viny tled, anel wahbis en yoand an\n",
      "Hound ?Ront tor, brey we oyfang Rhee so bithe nerind\n",
      "Theur onin tot tored nouy no urthet n \n",
      "----\n",
      "iter 2200, loss: 132.650003\n",
      "----\n",
      " .\n",
      "\n",
      "Cou mithe efer:sr be thetlld sird banlt irr tor hornour on ansas tonit lont\n",
      "Yore-, the f aocktar'ur to selde-\n",
      "\n",
      "Sfove yanthy yir arg yie cesatoucgos uravelesauis \n",
      "oo'h\n",
      "La Ggyhivg\n",
      "Sd s yade slilano g \n",
      "----\n",
      "iter 2300, loss: 131.567679\n",
      "----\n",
      " .rt muatient muvhy ponwous golea'e afyoC ondand o ceathe Sithe aGe or or bfit baker soV.\n",
      "UUne hhes bm fhatre cher, bevine par.?\n",
      "\n",
      "int:enr poo? coudirehengabs\n",
      "Thamceoe fothal cese any whe soreE, themlt  \n",
      "----\n",
      "iter 2400, loss: 130.347345\n",
      "----\n",
      " ekethe nor heors yhe mano ms;!\n",
      "Aus tl hes:.\n",
      "BTharvers ass pee thud bie wou sest wad ang wounind of witho\n",
      "\n",
      "tan, sed sey any, at thes.\n",
      "DCCARwTlals,\n",
      "An!,\n",
      "Ammas? So'ds,\n",
      "\n",
      "Wal'ee bale sour waD, thireqw wit  \n",
      "----\n",
      "iter 2500, loss: 129.223418\n",
      "----\n",
      " irere:.\n",
      "MNW Teute mald: iu mimle, un tou'ins\n",
      "grore uanf enive witrlo ar ans hace\n",
      "Thekc;r\n",
      "oNdennce hf waer wiie ied weus, wherer souin  mydle cet fout avedimbd iror-:vit wy toill, satte nouptat oute co \n",
      "----\n",
      "iter 2600, loss: 128.052898\n",
      "----\n",
      " ry hoo : asMI torts feaeeteln'ur inptu toon gath! hhe; avd,\n",
      "I vetost, Tan- whed lomeso ;iseve then\n",
      "\n",
      "CIRONNTNNUNUS:\n",
      "Seril aher faellkr po yorgh; lr arna; canv ouled sere seith whecis ou s higl;\n",
      "cerscou \n",
      "----\n",
      "iter 2700, loss: 127.199929\n",
      "----\n",
      " r gipetle\n",
      "God hof soor' hede ghaerer ilcerild !ruk.\n",
      ".\n",
      "OSSUS:\n",
      "\n",
      "oous; bill, buse yuthed to, Ges' thousgs mo, we\n",
      "so'd aw ruof gorrragYe ptore entitise banic,\n",
      "Sg eom iznt tcey thoo pikt, bl fe te. Motha x \n",
      "----\n",
      "iter 2800, loss: 126.441878\n",
      "----\n",
      "  shinu bony unguin,\n",
      "Foatting the shan dane wpof of hilg, ti; whe 'lesles shensefer ius noy,; hid sn wuto wh higoid oave ind therpalge s 'enmf waiis no wers cire mend'd weras in ir ty peom, fs thausus  \n",
      "----\n",
      "iter 2900, loss: 125.665837\n",
      "----\n",
      " whith\n",
      "Couk thinoni,e are tserit obse d oul't isithar bot ble hoce hou 'nnalt: Iuengame ribet.\n",
      "Theimqd thes, have fhe incer!\n",
      "\n",
      "CNOENUIU::\n",
      "Chic yerey\n",
      "Woremd sheer-, mirer honk mepathod: m therst the sldc \n",
      "----\n",
      "iter 3000, loss: 124.973012\n",
      "----\n",
      "  derly meln Cinr, cfo li;cive inssy HesI: A oe!\n",
      "Silme?;\n",
      "Ae, as hadey gaklo, tive CofdS\n",
      "Henwo Howidr:\n",
      "Sy in'ct.r rrath blcouum Icoud art harlett tors artind wie- ay th mesaip. warale tirt wa sho pocyop \n",
      "----\n",
      "iter 3100, loss: 124.622967\n",
      "----\n",
      " jall porpete wedptores.\n",
      "\n",
      "CrEIUSUS:\n",
      "To hay briue cethot pue.,\n",
      "Aok de's,e fousle diwh: Inatlinrwttime foag wha' hawe worrt film\n",
      "Than:\n",
      "Vout'ln yoate wit obat wag'y Ton fat thas you ;'ar' toureml iud\n",
      "dhe  \n",
      "----\n",
      "iter 3200, loss: 124.475749\n",
      "----\n",
      " th ou her't yo say cidary\n",
      "Wf y iat I.\n",
      "\n",
      "ONOMRLSE,::\n",
      "Soukt by anddwer and, dot to paroDtoom hurs-'l kizay\n",
      "Vuy than?\n",
      "Hhe tth thach to hies\n",
      "At th I hatd hirs, I w poa,dt thostow to neushe mnawe ufiat oul  \n",
      "----\n",
      "iter 3300, loss: 124.109614\n",
      "----\n",
      " ng the tiy chaveilol:!\n",
      "H mitin\n",
      "sNand famcend wetmyo.\n",
      "G:rAoncefrlle, that samlg af smas thand\n",
      "Mezt hesl te tacn\n",
      "Thes, bey yot oalgr out, lithardsag bamsdeld tl nout orlem:\n",
      "Puy gato hand thare mecerl' p \n",
      "----\n",
      "iter 3400, loss: 123.577791\n",
      "----\n",
      " my Iyet oudy; hird\n",
      "Cids, that dist brom,\n",
      "Wisethe bl imis anget:,\n",
      "Er,\n",
      "Indsich oren taabl upes hind, gurowil go torn ind\n",
      "Mocacs:ye, Sy ane\n",
      "sh tod diks goud Wore, beand tave omi chlagy thisn.\n",
      "\n",
      "CCICARTEE. \n",
      "----\n",
      "iter 3500, loss: 123.297476\n",
      "----\n",
      " sane woch ulle setaeestes-\n",
      "Ulat iccin-?\n",
      "Thd hiy ing isngs wilit, seche ob tien, lt'ris pcore wl roo tor,, wam.\n",
      "\n",
      "VLeLTAAd\n",
      "tn cerepigils fet serporin thald ratn oy Iind antUpereceesest, naslelt Tike srw \n",
      "----\n",
      "iter 3600, loss: 123.119300\n",
      "----\n",
      " mer, ouedune yon, wo thea thors\n",
      "Ie gols oind she gelk thame.\n",
      "\n",
      "IOAE\n",
      "AAUTI\n",
      "Thadenge nhlrad at then de con,\n",
      "Tut chit can.\n",
      "\n",
      "Ir:\n",
      "EER\n",
      "MMEEI\n",
      "HAve! yout lisdewot wavh cive feasts nof an hertqho yit vid djat's \n",
      "----\n",
      "iter 3700, loss: 122.754008\n",
      "----\n",
      " OFAOxA:\n",
      "MaSe-llirt andan ts lo angto  eath, wod des.\n",
      "DA\n",
      "K&RI,\n",
      "EUROUT:-Sy,\n",
      "Hamik.\n",
      "Gith cead Thit:\n",
      "Hebas,\n",
      "Goplan d toul yore wamld Shetis with sfou sishe shoud!\n",
      "\n",
      "RRLSLAn:\n",
      "Daun, wo bor motwim 't siis de  \n",
      "----\n",
      "iter 3800, loss: 122.171786\n",
      "----\n",
      " HRURF:\n",
      "AvAcX\n",
      "Alqdugs ond!:\n",
      "Ger is afer.\n",
      "\n",
      "Whcbu healco goors,\n",
      "Chklenll\n",
      "Wime sheel ghes on stems co ier dy sein',\n",
      "Andae; vach conat om mt etos poy wo y'd tathe thald f'aow mdeave\n",
      "ery or wats\n",
      "\n",
      "IxWhore:\n",
      "\n",
      " \n",
      "----\n",
      "iter 3900, loss: 121.952595\n",
      "----\n",
      " EI\n",
      "Be:oard,\n",
      "d a?,\n",
      "Sard syoure gid ?owin serther de bat we iy dovt, Hord lo tous, hs anfertr turd owedt lom Mures\n",
      "Tr tor sivle\n",
      "Sfeur mf alr hher\n",
      "\n",
      "Erot mengun'urerls Irfio omes: youn ools.\n",
      "Whe souu morg \n",
      "----\n",
      "iter 4000, loss: 121.438141\n",
      "----\n",
      "  yous hother\n",
      "Werarrj.\n",
      "Whame in wose dou hiee mh rersste ldfermomg rest praur;\n",
      "Fo dold io qfmarh: sker buts, yoy,\n",
      "Whe herkl of jaamd by Vill shing that toul ct, Msefo he, hatea. M, hive Pathe sfolhaGd  \n",
      "----\n",
      "iter 4100, loss: 120.815409\n",
      "----\n",
      " ey me whacs bered mart,\n",
      "Th ert\n",
      "Whe urthat, had tot dod tot dets aruxs or wor wiost\n",
      "Wnrild,\n",
      "Ser taat,\n",
      "Dikd womr thiende:\n",
      "W, alb yrud hougt thee:\n",
      "Wow wand ao the cow yer tad thectrot R iar it bei: ou, m \n",
      "----\n",
      "iter 4200, loss: 120.666677\n",
      "----\n",
      " n, boor Cerives, the ky anf yasto, eress been, hhad be. while\n",
      "Wouncod beicerte keal so hath\n",
      "Hr kyor hs Inuncasl\n",
      "Wawu the gy wover sismerrest worlethy me. hyincenybe hergsgatcleseN int thitfsind yod aj \n",
      "----\n",
      "iter 4300, loss: 120.690454\n",
      "----\n",
      " tor!\n",
      "\n",
      "MORETiIZ;\n",
      "The dar!,\n",
      "Bue Ins:\n",
      "Ir ridin-.\n",
      "\n",
      "A!N\n",
      "CGHTYI:WEerind 'hrrasz gid.\n",
      "\n",
      "GUOkENAN\n",
      "GHTUN\n",
      "ADUO\n",
      "MWA BRENSRH\n",
      "HE.\n",
      "AAAS:\n",
      "Bnd the dor indd bu tho ke beinos lwace notd stings ann nath'risl Vsprrbleashe \n",
      "----\n",
      "iter 4400, loss: 120.456571\n",
      "----\n",
      " hein tousereeus on eow ae falp wale ARsenone in luvelaMe thec nuy axstods 'n or sn'thace,\n",
      "Oe in dhith aaggord.\n",
      "\n",
      "foREvE I, buregefs.\n",
      "\n",
      "LRNCEC:CES:\n",
      "Chout this wim hacgcowy Woof meit hird core becerss ane \n",
      "----\n",
      "iter 4500, loss: 120.455897\n",
      "----\n",
      " , thangel:\n",
      "Vn stol then,\n",
      "Lw doren ine Io kits wey corthe, ciw,.\n",
      "\n",
      "'RaEn:\n",
      "Gagrgeor tat O? zo snkas tous onentor thon maderds\n",
      "dod mir, Euld haveplud I behll stathe and burr.\n",
      "\n",
      "VO$RfWind hithme is weat:\n",
      "He \n",
      "----\n",
      "iter 4600, loss: 119.978803\n",
      "----\n",
      "  megt yo the lyoe, bthe lowtd niy sherbors thare sune he and ur iy'l wor wnyellt eret lbthitha. are wpimethl' oCl no rawlo?\n",
      "Fint, asr, ao tou,\n",
      "\n",
      "flr nolll me aad as;\n",
      "AnD, ao hem tor, vis ither.\n",
      "\n",
      "hire:\n",
      " \n",
      "----\n",
      "iter 4700, loss: 119.510433\n",
      "----\n",
      " badawe dour wan he rorngt?\n",
      "\n",
      "AOlUT\n",
      "SDRCO\n",
      "ESCBWhI:\n",
      "Gre gous bs hovenge ghan this,,\n",
      "Apse wollam while sove?\n",
      "Thes iy Found in:\n",
      "Me; Mimi. Hvill thesucande gaanld I unelklury re fale the, Aegr lorass thic C \n",
      "----\n",
      "iter 4800, loss: 119.128296\n",
      "----\n",
      " eutn arese theo nher took seis ind,\n",
      "G lavw.\n",
      "I:\n",
      "Tove.\n",
      "\n",
      "FI IhA:\n",
      "\n",
      "oEOd Tf pater ofo teave satt Cay Mlait hondecj alas hal, Isthed ancer peas ie thid shat bid leet wanit han yome wheast inod, woe noup me  \n",
      "----\n",
      "iter 4900, loss: 118.925690\n",
      "----\n",
      " \n",
      "Gore,\n",
      "BmdwE&; zerimd wese the syout I boce coug?\n",
      "\n",
      "GNCOAHCESN:\n",
      "We he t mom hu, and anqerendmus,\n",
      "Thed of charen no dyeleis cyour ie se!My\n",
      "Wa ur thang ticpx, tois horgewe on litk frersenthan sur,\n",
      "\n",
      "uerek \n",
      "----\n",
      "iter 5000, loss: 118.490836\n",
      "----\n",
      " Thill I padit; oit hiths ild wecfas abdo hod yorcese alchplate hey Yore,\n",
      "Mot fins, hed thunn.\n",
      "\n",
      "SUOUSCGEDS\n",
      "TESCECTFTake rord wircind bris,\n",
      "Tyre hrech powes god hout iigr totito hoe d la hit;, Tlo ined  \n",
      "----\n",
      "iter 5100, loss: 118.265854\n",
      "----\n",
      " t.\n",
      "\n",
      "AELCDIEYT Rse toul thing ity thozis at; bf bwul gvare'd hy ind?\n",
      "\n",
      "LyATzHse hunlerder\n",
      "COdpirrt nove rsin sif.\n",
      "Tin ses lnme cimasse mit my bralyou selll heomy\n",
      "Nhan.\n",
      "\n",
      "DCOLIUHS:\n",
      "Loy so ,hon boun.\n",
      "\n",
      "odgy \n",
      "----\n",
      "iter 5200, loss: 117.905151\n",
      "----\n",
      "  Ir armriankthe hy poud yor bye ratas.\n",
      "BL bice youd men! goko tfale?\n",
      "The gorgiwh ly thes.\n",
      "\n",
      "Wharciplise, hirly endeure show woon, lotiend shis wilng.\n",
      "I mik lntor youlerend gerave eargik!: my or cippirr \n",
      "----\n",
      "iter 5300, loss: 117.866573\n",
      "----\n",
      " , yat hape hime,\n",
      "\n",
      "VaS:\n",
      "zapf ees oodir ny mers B!r ankeve shir ugyore Ton fend.\n",
      "\n",
      "GMCLATTIAS:\n",
      "This.\n",
      "Wyat thie, you nu katrenco.\n",
      "\n",
      "DCNCICIAR:\n",
      "ARy I hird fmofu jlid doris swele perthele ea ee hiily\n",
      "C'us'd  \n",
      "----\n",
      "iter 5400, loss: 117.986675\n",
      "----\n",
      " ury lathey hout bertiatd and, angd ond\n",
      "At; uers heang not\n",
      "Whelkanfery.\n",
      "\n",
      "GOYARA:\n",
      "Bla.\n",
      "\n",
      "BODYMN GGREH:\n",
      "Gou tidn the loat on meus ilderfiwly\n",
      "To I yofath wife,\n",
      "Hp terevhecs;\n",
      "Thou, an tari.\n",
      "Whor mecorgure c \n",
      "----\n",
      "iter 5500, loss: 118.214583\n",
      "----\n",
      " hapriendtrr he,,\n",
      "Ber the nosle.\n",
      "Hhet,\n",
      "What ul wheald carfod ar, th th: toursasd:\n",
      "Aon hame mle, pR hows ines th-r!;\n",
      "Bows dow pyti y ery dpevethascath teir, thers\n",
      "\n",
      "uyoE ine\n",
      "Tuu seame wathery Oobs;\n",
      "\n",
      "GKCE \n",
      "----\n",
      "iter 5600, loss: 117.949686\n",
      "----\n",
      " larle at midewiny bery and on oul ce lururirs a, boud nle; be gourserte lhle, as of athal.\n",
      "\n",
      "MUIDKRE:\n",
      "Bsergfir, foid there,\n",
      "Ar,\n",
      "\n",
      "Y mCyalb Sersexthrec ifacat erathar yove of lflu, hilt il:\n",
      "Fumd ptret'\n",
      "d \n",
      "----\n",
      "iter 5700, loss: 117.768073\n",
      "----\n",
      " IsRNTHil cr cibld ir she cagletn hime haedes ferr ond ono vadthi! norbmit.\n",
      "\n",
      "TONwAR Arl theuy,\n",
      "Tolle thd uy the dasesifome te thum s andor merse, yorrnde thalr seich, cerlpyou be?\n",
      "\n",
      "AUIZB\n",
      "GDCIGHHT:\n",
      "I ca \n",
      "----\n",
      "iter 5800, loss: 117.192426\n",
      "----\n",
      " l of tous bevtem',hur!\n",
      "Vfys worgh'd dorer arle tounw, fsenbertte why des:\n",
      "MI ins;ree st ool foth rneare soms uns my unth iel waeraclengonf hoop nordount tome gamek.\n",
      "\n",
      "QIIE:\n",
      "I Covqel: an the.\n",
      "Teat thir  \n",
      "----\n",
      "iter 5900, loss: 116.844925\n",
      "----\n",
      " the mecoschith wibrs;\n",
      "Dhe f tittoun, To efalliswy, mersod, erd rove!d word me thint tooy thy ig, nom maoll, bn chastin, ane wiu\n",
      ".\n",
      "\n",
      "BUKBRTAH:\n",
      "Wor bfdeous wo wis beee, ny you pord afd toosbeagl thed my  \n",
      "----\n",
      "iter 6000, loss: 116.855802\n",
      "----\n",
      " sghe fouf ay wenlnqut seresgo't Thas ,ous bist if stat hath to this mands ow she thes byeu; powy\n",
      "ye hof.\n",
      "Con that to ale.\n",
      "\n",
      "LREIGNIRI: Iouk th ky tho cofllo ghade,\n",
      "And Theratoud moocertald meitist\n",
      "Me c \n",
      "----\n",
      "iter 6100, loss: 116.972118\n",
      "----\n",
      " I;\n",
      ":Ir famthis.\n",
      "\n",
      "LUERL GCICRNIHTsy tharh'f ty thaiwt.\n",
      "Theh st hefwanr enelie,\n",
      "-vat tallend.\n",
      "Soncy\n",
      "I lrash-e;\n",
      "Ar thha githe, bomibs thl, I rlk hirk urvend wy bo bery Pofvond he nyous!\n",
      "Art ty efapoug be \n",
      "----\n",
      "iter 6200, loss: 117.457259\n",
      "----\n",
      " y poame bley.\n",
      "\n",
      "SwOCGNHI HIICKARI ROINFIAW\n",
      "\n",
      "'sand coud thist of dilymof in ciths tor:\n",
      "Ho; guel lly eases.\n",
      "Toe\n",
      "'se me bot, dink\n",
      "af enns win \n",
      "eard,\n",
      "Wher wom.\n",
      "Whe sil horane on therous\n",
      "Ind our, ar tas?\n",
      "Th \n",
      "----\n",
      "iter 6300, loss: 117.139838\n",
      "----\n",
      " rel, fhas youd bit mith ,ng,, then, :oun beast and seruom ay therethe an tous ame dom aod shyy the nime or of cofr! Cond hallo Troau,\n",
      "Oe Srore th so!\n",
      "Bun oun \n",
      "bfarerot.\n",
      "Thoy arle mekS.\n",
      "Anpues.\n",
      "\n",
      "yOORNR \n",
      "----\n",
      "iter 6400, loss: 117.141395\n",
      "----\n",
      " fou suoche wisepe, Id fsati, shueng\n",
      "Tf hid the the ous st cord thus,'bath\n",
      "\n",
      "BUOTw\n",
      "BRZGITTI I gore as bere thif, deill gint argrlt yol 'nsigheshim;\n",
      "Do aiy deoce!\n",
      "\n",
      "'HEGAN\n",
      "AKBGTElMr thet greid, ce os s, w \n",
      "----\n",
      "iter 6500, loss: 117.216741\n",
      "----\n",
      "  uwrmed sill hie's; fatatcof, burs- takif taon thed hom' in space, the m; whed,\n",
      "I methe mealile hlam, d alloung:\n",
      "The tpen,\n",
      "bheml Fathit him.\n",
      "\n",
      "RUHHM:\n",
      "A-IN DVCDInERD:\n",
      "Anerthor ae I thor.\n",
      "And mich het, h \n",
      "----\n",
      "iter 6600, loss: 117.133645\n",
      "----\n",
      " Th fotharo th thy icbe yowe, ,yol!\n",
      "\n",
      "KaRECETCSNHAr:\n",
      "Htoun for ifothe thang;\n",
      "BA noben!\n",
      "Horcot, firsbliche poms\n",
      "ale,\n",
      "Homtr iingens eogigt br weve cqcoveid cardirt ay yos te luno,\n",
      "Ank anllell.\n",
      "Fhores an c \n",
      "----\n",
      "iter 6700, loss: 117.096320\n",
      "----\n",
      " e his ar chand To thapr Foen-of ty urskinp Iadin.\n",
      "The meni'ce I Th this hersy ureof\n",
      "Ta som.\n",
      "Wang to srall.'\n",
      "dand daden cou how ke dersirk behulcd;\n",
      "An,\n",
      "Wou hed lore ind abd Ras an ssple,\n",
      "I ded;\n",
      "Thor be \n",
      "----\n",
      "iter 6800, loss: 116.946226\n",
      "----\n",
      "  buor bous ho mg:\n",
      "Outin:\n",
      "Whos whas hoagh las.\n",
      "\n",
      "AROD:\n",
      "Futs we tod sepurlanl wied,\n",
      "Bed wangr arthe the swis whousath lists cath dirfr, bords: mof waran nue harskervile\n",
      "OUt oue the lathe wead te dad tist \n",
      "----\n",
      "iter 6900, loss: 116.771628\n",
      "----\n",
      " d in thawk deur onle. wend ingild ce;'st I yol say'sulus,\n",
      "An thems hole.\n",
      "\n",
      "AULH GUEEEDYTI my ene so aveis,\n",
      "Or inpers.\n",
      "Fores, thom:\n",
      "Ond Oant hthe fove s acttead hard daen.-\n",
      "IuT!;\n",
      "Than at ;iwher.\n",
      "\n",
      "HhNR:  \n",
      "----\n",
      "iter 7000, loss: 116.515767\n",
      "----\n",
      " crossl\n",
      ";nth oun thouf lore in le beier on ave gt sus smont, Peir,\n",
      "Whangrsithocesrlavas'fy thus dott thees,\n",
      "Af he sefor Gy cost bins!\n",
      "Thos cowesk rascentat drethe fave ous rnoul berend, I fic- ings Ohi \n",
      "----\n",
      "iter 7100, loss: 116.129329\n",
      "----\n",
      " s:\n",
      "Whe vearcous the thor bathof wuse'd fre marstouns sheo denon ltay thl noy ours\n",
      "O hay bresthes,\n",
      "Qha feult sian harcisd lifce,\n",
      "Amtr ppy wils,\n",
      "SnI maver ly matnde ssounl.\n",
      "\n",
      "foRs\n",
      "\n",
      "BORG:\n",
      "Thar he?\n",
      "\n",
      "FAy dI \n",
      "----\n",
      "iter 7200, loss: 116.159607\n",
      "----\n",
      " es\n",
      "Our mfek wansen:\n",
      "Aral tat or sef ane io midh mar, no bit,\n",
      "sa th lel:\n",
      "re thie Shomien tok th behur and; il dow nad bthing\n",
      "Whes comlesd th tootitr s herterP, andd breisrandey be mius.\n",
      "Af: ura, an wie \n",
      "----\n",
      "iter 7300, loss: 116.053010\n",
      "----\n",
      " d andeve thew shas is\n",
      "f owva dpeeach, pieg yob.\n",
      "\n",
      "AORF:\n",
      "Hure Cowline fokr thap!\n",
      "Turis tigh the sres dowawe yous yor mas yous seikling;\n",
      "And:\n",
      "\n",
      " ircpey mond ou cfove houd wave charnem, thof wo this oo hy  \n",
      "----\n",
      "iter 7400, loss: 115.920279\n",
      "----\n",
      " rr tuig our hurcome tarsow whe esty heme in stilderer pall Fe thiint.\n",
      "\n",
      "KINA GAGGNUAT:\n",
      "Bigl win omp tan larherd brenn tout abewing;\n",
      "Fo toipil;\n",
      "Hy us,\n",
      ": ifrdy uled docd ond ,ouls lamd os werte laver cow \n",
      "----\n",
      "iter 7500, loss: 115.663236\n",
      "----\n",
      " you.\n",
      ",\n",
      "Or on ot ralk ind ir ith my fond,\n",
      "Womit rrouranPs!\n",
      "\n",
      "UmOOch on hous Gort the bito cind thamr hean of bokes tor lalulet,\n",
      "Rour eglt Bacy yin dey indevingad\n",
      "Gt phed tit ingt I thes dirty mndme laen \n",
      "----\n",
      "iter 7600, loss: 115.443211\n",
      "----\n",
      "  ano'r Iryeusl. deind wad weagl waame derve warty soret core urrle in-the fote thit soud wiere gor sesove lerwiny,\n",
      "Thaf theun sfast af tare;\n",
      "We seant wor 'fsureerel\n",
      "\n",
      "MRKE E::\n",
      "Du. hom, os the lotd.\n",
      "Fel \n",
      "----\n",
      "iter 7700, loss: 115.421515\n",
      "----\n",
      "  lllave sif Whow bist I and bith wighiacs Rade;\n",
      "ofr rave the neftwel yoke not kinh foot;seBe lape bare\n",
      "Fpopuly tht s, lats miced,\n",
      "Lais ant Heshe, be iss aoJ Ely Rarsrerngad oy if bostFy I urr esr harc \n",
      "----\n",
      "iter 7800, loss: 115.263504\n",
      "----\n",
      " it'L.\n",
      "I in lllintisssst, blla, the Mi-f astes'sk Ffronelr?\n",
      "Hes borle cofby ar af an marm wintte benthes leing haraltabe,\n",
      "Intwrde hes rave than'ds hery Is, at os thand\n",
      "wes hall;\n",
      "No stist fusher angeriv \n",
      "----\n",
      "iter 7900, loss: 115.177144\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('linux_input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 50 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# model parameters\n",
    "#Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "#Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "Wxz = np.random.randn(hidden_size, vocab_size)*0.01 # input a actualizacion\n",
    "Whz = np.random.randn(hidden_size, hidden_size)*0.01 # hidden a actualizacion\n",
    "Wxn = np.random.randn(hidden_size, vocab_size)*0.01 \n",
    "Whn = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "Wxr = np.random.randn(hidden_size, vocab_size)*0.01\n",
    "Whr = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "br = np.zeros((hidden_size, 1))\n",
    "bn = np.zeros((hidden_size, 1))\n",
    "bz = np.zeros((hidden_size, 1)) # bias actualizacion\n",
    "#bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, zs, rs, ys, ps, hcs = {}, {}, {}, {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    zs[t] = sigmoid(np.dot(Wxz, xs[t]) + np.dot(Whz, hs[t - 1]) + bz)\n",
    "    rs[t] = sigmoid(np.dot(Wxr, xs[t]) + np.dot(Whr, hs[t - 1] + br))\n",
    "    hcs[t] = np.tanh(np.dot(Wxn, xs[t]) + rs[t] * np.dot(Whn, hs[t - 1]) + bn) # hidden candidate state\n",
    "    hs[t] = (1 - zs[t]) * hs[t - 1] + zs[t] * hcs[t] # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxz, dWhz, dbz = np.zeros_like(Wxz), np.zeros_like(Whz), np.zeros_like(bz)\n",
    "  # (r) Reset gate\n",
    "  dWxr, dWhr, dbr = np.zeros_like(Wxr), np.zeros_like(Whr), np.zeros_like(br)\n",
    "  # (n) Candidate state\n",
    "  dWxn, dWhn, dbn = np.zeros_like(Wxn), np.zeros_like(Whn), np.zeros_like(bn)\n",
    "  dWhy, dby = np.zeros_like(Why), np.zeros_like(by)\n",
    "\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhc = dh * zs[t]\n",
    "    dz = dh * (hcs[t] - hs[t - 1])\n",
    "    dh_prev_h = dh * (1 - zs[t])\n",
    "    #dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dzraw = (zs[t] * (1 - zs[t])) * dz\n",
    "    dhcraw = (1 - hcs[t]**2) * dhc\n",
    "\n",
    "    #dbh += dhraw\n",
    "    dbn += dhcraw\n",
    "    dWxn += np.dot(dhcraw, xs[t].T)\n",
    "\n",
    "    dr = dhcraw * np.dot(Whn, hs[t - 1])\n",
    "    dh_prev_hcs = np.dot(Whn.T, dhcraw * rs[t])\n",
    "    dWhn += np.dot(dhcraw * rs[t], hs[t - 1].T)\n",
    "\n",
    "    dbz += dzraw\n",
    "    dWxz += np.dot(dzraw, xs[t].T)\n",
    "\n",
    "    dh_prev_z = np.dot(Whz.T, dzraw)\n",
    "    dWhz += np.dot(dzraw, hs[t - 1].T)\n",
    "\n",
    "    drraw = (rs[t] * (1 - rs[t])) * dr\n",
    "    dbr += drraw\n",
    "    dWxr += np.dot(drraw, xs[t].T)\n",
    "\n",
    "    dh_prev_r = np.dot(Whr.T, drraw) \n",
    "    dWhr += np.dot(drraw, hs[t - 1].T)\n",
    "\n",
    "    dhnext = dh_prev_h + dh_prev_hcs + dh_prev_z + dh_prev_r\n",
    "  for dparam in [dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxn, dWhn, dbn, dWhy, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxn, dWhn, dbn, dWhy, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    r = sigmoid(np.dot(Wxr, x) + np.dot(Whr, h) + br) # Cuánto del estado anterior se olvida\n",
    "    z = sigmoid(np.dot(Wxz, x) + np.dot(Whz, h) + bz) # Cuánto del estado anterior se mantiene\n",
    "    h_can = np.tanh(np.dot(Wxn, x) + r * np.dot(Whn, h) + bn)\n",
    "    h = (1 - z) * h + z * h_can\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxz, mWhz, mbz = np.zeros_like(Wxz), np.zeros_like(Whz), np.zeros_like(bz)\n",
    "# (r) Reset gate\n",
    "mWxr, mWhr, mbr = np.zeros_like(Wxr), np.zeros_like(Whr), np.zeros_like(br)\n",
    "# (n) Candidate state\n",
    "mWxn, mWhn, mbn = np.zeros_like(Wxn), np.zeros_like(Whn), np.zeros_like(bn)\n",
    "\n",
    "# (Output)\n",
    "mWhy, mby = np.zeros_like(Why), np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while (n < 8000):\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxn, dWhn, dbn, dWhy, dby, hprev = lossFun(inputs, targets, hprev)  \n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxz, Whz, bz,\n",
    "                Wxr, Whr, br,\n",
    "                Wxn, Whn, bn,\n",
    "                Why, by], \n",
    "                                [dWxz, dWhz, dbz,\n",
    "                 dWxr, dWhr, dbr,\n",
    "                 dWxn, dWhn, dbn,\n",
    "                 dWhy, dby], \n",
    "                                [mWxz, mWhz, mbz,\n",
    "              mWxr, mWhr, mbr,\n",
    "              mWxn, mWhn, mbn,\n",
    "              mWhy, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9950c-e831-42d0-9484-a91519d31eeb",
   "metadata": {},
   "source": [
    "<p>En este caso, con seq_length = 50, la implementación de GRU no se estanca en cuanto a la reducción de su pérdida como la RNN común. Esto tiene sentido, ya que la RNN falla en las predicciones a largo plazo, mientras que la GRU puede seguir aprendiendo. También </p>\n",
    "\n",
    "<p>Ahora, si bajamos el seq_length a 25, la RNN debería comportarse mejor.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
